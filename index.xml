<?xml version="1.0" encoding="UTF-8" ?>
<rss version="2.0">
    <channel>
      <title>🫧 11ze</title>
      <link>https://wangze.tech</link>
      <description>Last 10 notes on 🫧 11ze</description>
      <generator>Quartz -- quartz.jzhao.xyz</generator>
      <item>
    <title>28｜Pika：如何基于 SSD 实现大容量 Redis？</title>
    <link>https://wangze.tech/28%EF%BD%9CPika%EF%BC%9A%E5%A6%82%E4%BD%95%E5%9F%BA%E4%BA%8E-SSD-%E5%AE%9E%E7%8E%B0%E5%A4%A7%E5%AE%B9%E9%87%8F-Redis%EF%BC%9F</link>
    <guid>https://wangze.tech/28%EF%BD%9CPika%EF%BC%9A%E5%A6%82%E4%BD%95%E5%9F%BA%E4%BA%8E-SSD-%E5%AE%9E%E7%8E%B0%E5%A4%A7%E5%AE%B9%E9%87%8F-Redis%EF%BC%9F</guid>
    <description> 基于大内存实现大容量 Redis 实例的潜在问题 内存快照 RDB 生成和恢复效率低 主从节点全量同步时长增加、缓冲区易溢出 Pika 键值数据库 设计目标 一、单实例可以保存大容量数据，同时避免实例恢复和主从同步时的潜在问题 二、和 Redis 数据类型保持兼容，可以平滑迁移到 Pika 上 整体架构 网络框架 Pika 线程模块 多线程 一个请求分发线程 DispatchThread 一组工作线程 WorkerThread 一个线程池 ThreadPool Nemo 存储模块 实现 Pika 和 Redis 的数据兼容 List Set Hash Sorted Set 不用修改业务应用中操作 Redis 的代码 RocksDB RocksDB 写入数据的基本流程 基于 SSD 保存数据 是一个持久化键值数据库 保存数据 使用两小块内存空间 Memtable1 和 Memtable2 交替缓存写入的数据 大小可设置 max_write_buffer_number 控制写限速 其中一块写满后，RocksDB 把数据以文件的形式快速写入底层的 SSD 读取数据 先在 Member 中查询，查询不到再到数据文件中查询 避免了内存快照的生成和恢复问题 在把数据写入 Memtable 时，也会把命令操作写到 binlog 文件中。 binlog 机制 实现增量命令同步 节省了内存，避免缓冲区溢出 其他优势 实例重启快 主从库执行全量同步风险低，不受内存缓冲区大小的限制 不足 性能比用内存低 多线程模型一定程度上弥补从 SSD 存取数据造成的性能损失 写 binlog 时影响性能 降低性能影响的建议 利用 Pika 的多线程模型，增加线程数量，提升 Pika 的并发请求处理能力 为 Pika 配置高配的 SSD，提升 SSD 自身的访问性能 工具 使用 aof_to_pika 命令迁移 Redis 数据到 Pika 中 Github｜Pika .</description>
    <pubDate>Fri, 23 Feb 2024 15:00:46 GMT</pubDate>
  </item><item>
    <title>29｜无锁的原子操作：Redis 如何应对并发访问？</title>
    <link>https://wangze.tech/29%EF%BD%9C%E6%97%A0%E9%94%81%E7%9A%84%E5%8E%9F%E5%AD%90%E6%93%8D%E4%BD%9C%EF%BC%9ARedis-%E5%A6%82%E4%BD%95%E5%BA%94%E5%AF%B9%E5%B9%B6%E5%8F%91%E8%AE%BF%E9%97%AE%EF%BC%9F</link>
    <guid>https://wangze.tech/29%EF%BD%9C%E6%97%A0%E9%94%81%E7%9A%84%E5%8E%9F%E5%AD%90%E6%93%8D%E4%BD%9C%EF%BC%9ARedis-%E5%A6%82%E4%BD%95%E5%BA%94%E5%AF%B9%E5%B9%B6%E5%8F%91%E8%AE%BF%E9%97%AE%EF%BC%9F</guid>
    <description> 原子操作 单命令操作 多个操作在 Redis 中实现成一个操作（如改源码） INCR/DECR 命令 以原子性方式执行 Lua 脚本 redis-cli —eval {lua.script} {keys}, {args} 要避免把不需要做并发控制的操作写入脚本 并发访问中需要控制的操作 读取 - 修改 - 写回操作（RMW） .</description>
    <pubDate>Fri, 23 Feb 2024 15:00:46 GMT</pubDate>
  </item><item>
    <title>30｜如何使用 Redis 实现分布式锁？</title>
    <link>https://wangze.tech/30%EF%BD%9C%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-Redis-%E5%AE%9E%E7%8E%B0%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%EF%BC%9F</link>
    <guid>https://wangze.tech/30%EF%BD%9C%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-Redis-%E5%AE%9E%E7%8E%B0%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%EF%BC%9F</guid>
    <description>单机版 用一个变量表示：0 没有线程获取到锁；1 有线程获取到锁 分布式锁 锁变量需要有一个共享存储系统来维护 如果为了效率，可以使用单节点，缺点是允许锁偶尔失效，优点是简单效率高 业务对结果要求非常严格，为了正确性，使用 Redlock，缺点是比较重，部署成本高 基于单个节点 加锁 SET lock_key unique_value NX [EX seconds | PX milliseconds] key 不存在时会被创建 key 存在，不做任何赋值操作 例：SET lock_key unique_value NX PX 10000 加锁成功后设置有效期 将上述操作写进 Lua 脚本作为一个原子操作 释放锁 比较锁变量的 unique_value 是否相等，避免误释放 unique_value：随机值，唯一，和其他客户端作区分 使用 Lua 脚本保证原子性 例：redis-cli —eval unlock.</description>
    <pubDate>Fri, 23 Feb 2024 15:00:46 GMT</pubDate>
  </item><item>
    <title>31｜事务机制｜Redis 能实现 ACID 属性吗？</title>
    <link>https://wangze.tech/31%EF%BD%9C%E4%BA%8B%E5%8A%A1%E6%9C%BA%E5%88%B6%EF%BD%9CRedis-%E8%83%BD%E5%AE%9E%E7%8E%B0-ACID-%E5%B1%9E%E6%80%A7%E5%90%97%EF%BC%9F</link>
    <guid>https://wangze.tech/31%EF%BD%9C%E4%BA%8B%E5%8A%A1%E6%9C%BA%E5%88%B6%EF%BD%9CRedis-%E8%83%BD%E5%AE%9E%E7%8E%B0-ACID-%E5%B1%9E%E6%80%A7%E5%90%97%EF%BC%9F</guid>
    <description>事务命令 MULTI：开启一个事务 EXEC：提交事务，从命令队列中去除提交的操作命令，进行实际执行 DISCARD：放弃一个事务，清空命令队列 只是清空，起不到回滚的作用 WATCH：检测一个或多个键的值在事务执行期间是否发生变化，如果发生变化，那么当前事务放弃执行 Redis 的事务机制 可以保证一致性和隔离性，无法保证持久性（非必要） 原子性 命令语法有误时，得不到保证 不存在的命令一开始就会被记录错误 无法得到保证的原因：命令和操作的数据类型不匹配，但 Redis 实例没检查出错误并开始执行 预防建议：严格按照 Redis 的命令规范进行程序开发，并且通过 code review 确保命令的正确性 实例发生故障，且 Redis 使用了 RDB 机制 RDB 不会在事务执行时执行，也就不会记录下事务执行了一部分的结果 存在一种情况无法保证原子性：如果事务执行完成，还没执行 RDB 快照，此时发生故障，会丢失事务修改的数据 其他情况，事务都可以原子性执行 前提：执行事务的 EXEC 命令时，Redis 实例发生了故障，导致事务执行失败 开启 AOF 日志 只会有部分的事务操作被记录到 AOF 日志中 使用 redis-check-aof 工具检查 AOF 日志文件 工具会把未完成的事务操作从 AOF 日志中去除 这时再使用 AOF 恢复实例，失败的事务操作不会再被执行 事务使用建议 配合 Pipeline 使用 隔离性由服务端保证，此时不需要使用 WATCH WATCH 的使用场景 WATCH key，读取 key，修改 key，写回 .</description>
    <pubDate>Fri, 23 Feb 2024 15:00:46 GMT</pubDate>
  </item><item>
    <title>32｜Redis 主从同步与故障切换，有哪些坑？</title>
    <link>https://wangze.tech/32%EF%BD%9CRedis-%E4%B8%BB%E4%BB%8E%E5%90%8C%E6%AD%A5%E4%B8%8E%E6%95%85%E9%9A%9C%E5%88%87%E6%8D%A2%EF%BC%8C%E6%9C%89%E5%93%AA%E4%BA%9B%E5%9D%91%EF%BC%9F</link>
    <guid>https://wangze.tech/32%EF%BD%9CRedis-%E4%B8%BB%E4%BB%8E%E5%90%8C%E6%AD%A5%E4%B8%8E%E6%95%85%E9%9A%9C%E5%88%87%E6%8D%A2%EF%BC%8C%E6%9C%89%E5%93%AA%E4%BA%9B%E5%9D%91%EF%BC%9F</guid>
    <description>1. 主从数据不一致 因为主从数据是异步复制 a）使用外部监控程序对比主从库复制进度，不让客户端从落后的从库中读取数据 开发一个工具： 基于 INFO replication 命令查看主库接收写命令的进度信息（master_repl_offset）和从库复制写命令的进度信息（slave_repl_offset） 比较两者大小，大于我们预设的阈值则不让客户端和此从库连接 b）保证主从间的网络链接状况良好 2.</description>
    <pubDate>Fri, 23 Feb 2024 15:00:46 GMT</pubDate>
  </item><item>
    <title>33｜脑裂：一次奇怪的数据丢失</title>
    <link>https://wangze.tech/33%EF%BD%9C%E8%84%91%E8%A3%82%EF%BC%9A%E4%B8%80%E6%AC%A1%E5%A5%87%E6%80%AA%E7%9A%84%E6%95%B0%E6%8D%AE%E4%B8%A2%E5%A4%B1</link>
    <guid>https://wangze.tech/33%EF%BD%9C%E8%84%91%E8%A3%82%EF%BC%9A%E4%B8%80%E6%AC%A1%E5%A5%87%E6%80%AA%E7%9A%84%E6%95%B0%E6%8D%AE%E4%B8%A2%E5%A4%B1</guid>
    <description>脑裂 指在主从集群中，同时有两个主节点，都能接收写请求 影响：客户端不知道该往哪个主节点写入数据，结果不同客户端往不同的主节点写数据，严重的会导致数据丢失 数据丢失排查过程 确认是不是数据同步出现问题 主库的数据未同步到从库且发生了故障，从库升级为主库，未同步的数据丢失 可以通过计算 master_repl_offset 和 slave_repl_offset 的差值做判断 排查客户端的操作日志，发现脑裂现象 在主从切换后的一段时间内，有客户端仍然在和原主库通信，并没有和升级的新主库进行交互 原主库假故障导致的脑裂 采用哨兵机制，如果超过预设数量的哨兵实例和主库的心跳都超时，才会把主库判断为客观下限，然后哨兵开始执行主从切换，切换完成后客户端会和新主库通信 和主库部署在同一台服务器上的其他程序临时占用了大量资源（例如 CPU 资源），导致主库资源使用受限，短时间内无法响应心跳。其它程序不再使用资源时，主库又恢复正常 主库自身遇到了阻塞的情况，例如，处理 bigkey 或是发生内存 swap，短时间内无法响应心跳，等主库阻塞解除后，又恢复正常的请求处理了 复习下 19｜波动的响应延迟：如何应对变慢的 Redis？（下） 中总结的导致实例阻塞的原因 脑裂应对方案 min-slaves-to-write：设置主库能进行数据同步的最少从库数量 min-slaves-max-lag：设置主从库间进行数据复制时，从库给主库发送 ACK 消息的最大延迟（以秒为单位） 搭配两个配置项，假设为 N 和 T，主库连接的从库中至少有 N 个从库，和主库进行数据复制时的 ACK 消息延迟不能超过 T 秒，否则，主库就不会再接收客户端的请求 只是减少数据的丢失 建议：假设从库有 K 个，将 min-slaves-to-write 设置为 K/2+1（如果 K = 1，就设为 1），将 min-slaves-max-lag 设置为十几秒（如 10～20s），在这个配置下，如果有一半以上的从库和主库进行的 ACK 消息延迟超过十几秒，我们就禁止主库接收客户端写请求 .</description>
    <pubDate>Fri, 23 Feb 2024 15:00:46 GMT</pubDate>
  </item><item>
    <title>35｜Codis VS Redis Cluster：我该选择哪一个集群方案？</title>
    <link>https://wangze.tech/35%EF%BD%9CCodis-VS-Redis-Cluster%EF%BC%9A%E6%88%91%E8%AF%A5%E9%80%89%E6%8B%A9%E5%93%AA%E4%B8%80%E4%B8%AA%E9%9B%86%E7%BE%A4%E6%96%B9%E6%A1%88%EF%BC%9F</link>
    <guid>https://wangze.tech/35%EF%BD%9CCodis-VS-Redis-Cluster%EF%BC%9A%E6%88%91%E8%AF%A5%E9%80%89%E6%8B%A9%E5%93%AA%E4%B8%80%E4%B8%AA%E9%9B%86%E7%BE%A4%E6%96%B9%E6%A1%88%EF%BC%9F</guid>
    <description>Codis 集群 Codis 集群的架构和关键组件图 codis server 进行了二次开发的 Redis 实例，其中增加了额外的数据结构，支持数据迁移操作，主要负责处理具体的数据读写请求 codis proxy 接收客户端请求，并把请求转发给 codis server Zookeeper 集群 保存集群元数据，例如数据位置信息和 codis proxy 信息 也可以换用 etcd 或本地文件系统保存元数据信息 etcd 是一个分布式键值对存储 codis dashboard 和 codis fe 共同组成集群管理工具 codis dashboard 负责执行集群管理工作，包括增删 codis server、codis proxy 和进行数据迁移 codis fe 负责提供 dashboard 的 Web 操作界面，便于进行集群管理 Codis 处理请求 先使用 codis dashboard 设置 codis server 和 codis proxy 的访问地址 客户端直接和 proxy 建立连接，不用修改客户端，和访问单实例 Redis 没区别 proxy 接收到请求，查询请求数据和 codis server 的映射关系，转给相应的 server 处理，最后通过 proxy 把数据返回给客户端 处理流程图 Codis 关键技术原理 集群里的数据分布 集群一共有 1024 个 Slot，编号依次是 0 到 1023 可以手动，也可以通过 codis dashboard 进行自动分配 客户端要读写数据时，使用 CRC32 算法计算数据 key 的哈希值，把哈希值对 1024 取模得到对应 Slot 的编号 CRC32(key) % 1024 = n 即可知道数据保存在哪个 server 上 数据路由表 指 Slot 和 codis server 的映射关系 在 codis dashboard 分配好路由表后会把路由表发送给 codis proxy，同时也会保存在 Zookeeper 中，codis proxy 会把路由表缓存在本地 路由表的分配和使用过程 集群扩容和数据迁移 增加 codis server 启动新的 codis server，将它加入集群 把部分数据迁移到新的 server Codis 集群按照 Slot 的粒度进行数据迁移 在源 server 上，Codis 从要迁移的 Slot 中随机选择一个数据，发送给目的 server 目的 server 确认收到数据后，会给源 server 返回确认消息。这时，源 server 会在本地将刚才迁移的数据删除 第一步和第二步就是单个数据的迁移过程。Codis 会不断重复这个迁移过程，直到要迁移的 Slot 中的数据全部迁移完成 支持两种迁移模式 同步迁移 阻塞，此时源 server 无法处理新的请求操作 异步迁移 非阻塞，迁移的数据会被设置为只读，不会出现数据不一致的问题 对于 bigkey，采用拆分指令的方式进行迁移：对 bigkey 的每个元素，用一条指令进行迁移 会给目的 server 上被迁移中的 bigkey 设置临时过期时间，如果迁移过程发生故障，不会影响迁移的原子性，完成迁移后删除设置的临时过期时间 可以通过异步迁移命令 SLOTSMGRTTAGSLOT-ASYNC 的参数 numkeys 设置每次迁移的 key 数量 增加 codis proxy 启动新的 proxy 通过 codis dashboard 把 proxy 加入集群即可 Codis 集群可靠性 codis server 给每个 server 配置从库，并使用哨兵机制进行监控 此时每个 server 成为一个 server group，都是一主多从 server group 的 Codis 集群架构图 codis proxy 和 Zookeeper 搭配使用 有超过半数的 Zookeeper 实例可以正常工作，Zookeeper 集群就可以提供服务 proxy 故障只需重启，然后通过 codis dashboard 从 Zookeeper 集群获取路由表即可恢复服务 切片集群方案选择建议 Codis 和 Redis Cluster 的区别 从稳定性和成熟度，选 Codis 从业务应用客户端兼容性，选 Codis 从数据迁移性能纬度看，选 Codis 从使用 Redis 新命令和新特性，选 Redis Cluster Codis server 是基于开源的 Redis 3.</description>
    <pubDate>Fri, 23 Feb 2024 15:00:46 GMT</pubDate>
  </item><item>
    <title>36｜Redis 支撑秒杀场景的关键技术和实践都有哪些？</title>
    <link>https://wangze.tech/36%EF%BD%9CRedis-%E6%94%AF%E6%92%91%E7%A7%92%E6%9D%80%E5%9C%BA%E6%99%AF%E7%9A%84%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF%E5%92%8C%E5%AE%9E%E8%B7%B5%E9%83%BD%E6%9C%89%E5%93%AA%E4%BA%9B%EF%BC%9F</link>
    <guid>https://wangze.tech/36%EF%BD%9CRedis-%E6%94%AF%E6%92%91%E7%A7%92%E6%9D%80%E5%9C%BA%E6%99%AF%E7%9A%84%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF%E5%92%8C%E5%AE%9E%E8%B7%B5%E9%83%BD%E6%9C%89%E5%93%AA%E4%BA%9B%EF%BC%9F</guid>
    <description>特征 瞬时并发访问量非常高 读多写少 秒杀场景的所有环节 活动前 不需要 Redis 尽量把商品详情页页面元素静态化，然后使用前端 CDN 或浏览器缓存把元素缓存起来 活动开始 Redis 参与的两个环节 使用 Redis 保存库存量，不交给数据库做库存扣减 如果把库存扣减在数据库执行，会带来两个问题 额外的开销。Redis 中保存了库存量，而库存量的最新值又是数据库在维护，所以数据库更新后，还需要和 Redis 进行同步，这个过程增加了额外的操作逻辑，也带来了额外的开销。 数据库处理较慢，不能及时更新库存余量，导致大量库存查验请求读取到 Redis 中的旧库存值，此时会出现下单数量大于实际的库存量，导致超售 最后在数据库处理订单 订单处理涉及支付、商品出库、物流等多个关联操作，要保证处理的事务性，需要在数据库中完成 订单处理时的请求压力已经不大 活动结束后 不需要 Redis 库存数据保存方式 使用切片集群，用不同实例保存不同商品的库存 每个商品用一个 Hash 类型的键值对保存 key: itemID value: {total: N, ordered: M} 先用 CRC 算法计算不同商品 key 对应的 Slot，然后，在分配 Slot 和实例对应关系时，才能把不同秒杀商品对应的 Slot 分配到不同实例上保存 基于原子操作支撑秒杀场景 使用 Lua 脚本执行库存查验和库存扣减操作，保证原子性 基于分布式锁来支撑秒杀场景 先让客户端向 Redis 申请分布式锁，只有拿到锁的客户端才能执行库存查验和库存扣减 这样可以在争夺分布式锁时过滤掉大量的秒杀请求 库存查验和扣减也不用使用原子操作了，因为多个并发客户端只有一个客户端能够拿到锁，已经保证了客户端并发访问的互斥性 建议 使用切片集群中的不同实例，分别保存分布式锁和商品库存信息 减轻保存库存信息的实例的压力 把秒杀商品的库存信息用单独的实例保存，不要和日常业务系统的数据保存在同一个实例上，避免干扰业务系统的正常运行 其他环节 前端静态页面的设计 请求拦截和流控 使用黑名单禁止恶意 IP 进行访问；限流；等等 库存信息过期时间处理 Redis 中的库存信息 = 数据库的缓存，不给 Reids 的库存信息设置过期时间 数据库订单异常处理 增加订单重试功能 .</description>
    <pubDate>Fri, 23 Feb 2024 15:00:46 GMT</pubDate>
  </item><item>
    <title>37｜数据分布优化：如何应对数据倾斜？</title>
    <link>https://wangze.tech/37%EF%BD%9C%E6%95%B0%E6%8D%AE%E5%88%86%E5%B8%83%E4%BC%98%E5%8C%96%EF%BC%9A%E5%A6%82%E4%BD%95%E5%BA%94%E5%AF%B9%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%EF%BC%9F</link>
    <guid>https://wangze.tech/37%EF%BD%9C%E6%95%B0%E6%8D%AE%E5%88%86%E5%B8%83%E4%BC%98%E5%8C%96%EF%BC%9A%E5%A6%82%E4%BD%95%E5%BA%94%E5%AF%B9%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%EF%BC%9F</guid>
    <description>数据量倾斜 在某些情况下，实例上的数据分布不均衡，某个实例上的数据特别多 bigkey 集合类型的 bigkey 如果是集合类型，可以拆分成很多个小的集合类型数据，分散保存在不同的实例上 比如通过 ID 范围拆分 避免 bigkey Slot 分配不均衡 手动迁移 Redis Cluster 的 Slot cluster slots 命令查看 Slot 分配情况 可用命令 CLUSTER SETSLOT：使用不同的选项进行三种设置，分别是设置 Slot 要迁入的目标实例，Slot 要迁出的源实例，以及 Slot 所属的实例 CLUSTER GETKEYSINSLOT：获取某个 Slot 中一定数量的 key。 MIGRATE：把一个 key 从源实例实际迁移到目标实例 手动迁移 Codis 的 Slot codis-admin —dashboard=ADDR -slot-action —create —sid=300 —gid=6 Hash Tag 指加在键值对 key 中的一对花括号 {} 如果有 Hash Tag，客户端在计算 key 的 CRC16 值时，只对 Hash Tag 花括号中的 key 内容进行计算 相同 Hash Tag 的数据会被映射到相同的 Slot 上 使用场景：用在 Redis Cluster 和 Codis 中，支持事务操作和范围查询 因为 Redis Cluster 和 Codis 本身不支持跨实例的事务操作 建议：不使用 Hash Tag，在客户端执行事务操作和范围查询 数据访问倾斜 虽然每个集群实例上的数据量相差不大，但是某个实例上的数据是热点数据，被访问得非常频繁 根本原因：实例存在热点数据 应对方法 只读的热点数据 具体做法：将热点数据复制多分，每个副本 key 增加一个随机前缀，映射到不同的实例的 Slot 中 有读有写的热点数据 给实例本身增加资源 比如配置更高的机器 集群的实例资源配置建议 在构建切片集群时，尽量使用大小配置相同的实例（例如实例内存配置保持相同），可以避免因实例资源不均衡而在不同实例上分配不同数量的 Slot .</description>
    <pubDate>Fri, 23 Feb 2024 15:00:46 GMT</pubDate>
  </item><item>
    <title>38｜通信开销：限制 Redis Cluster 规模的关键因素</title>
    <link>https://wangze.tech/38%EF%BD%9C%E9%80%9A%E4%BF%A1%E5%BC%80%E9%94%80%EF%BC%9A%E9%99%90%E5%88%B6-Redis-Cluster-%E8%A7%84%E6%A8%A1%E7%9A%84%E5%85%B3%E9%94%AE%E5%9B%A0%E7%B4%A0</link>
    <guid>https://wangze.tech/38%EF%BD%9C%E9%80%9A%E4%BF%A1%E5%BC%80%E9%94%80%EF%BC%9A%E9%99%90%E5%88%B6-Redis-Cluster-%E8%A7%84%E6%A8%A1%E7%9A%84%E5%85%B3%E9%94%AE%E5%9B%A0%E7%B4%A0</guid>
    <description> Redis Cluster 实例间以 Gossip 协议进行通信的机制 Gossip 协议的工作原理 两个实例间进行 PING、PONG 消息传递的情况 每个实例默认每秒从集群中随机挑选一些实例，把 PING 消息发送给挑选出来的实例，用来检测这些实例是否在线，并交换彼此的状态信息 PING 消息中封装了发送消息的实例自身的状态信息、部分其它实例的状态信息，以及 Slot 映射表 一个实例在接收到 PING 消息后，会给发送 PING 消息的实例，发送一个 PONG 消息。PONG 消息包含的内容和 PING 消息一样 Gossip 消息大小 PING 和 PONG 消息的消息体都大约 12KB PING 消息中带有一个长度为 16384 bit 的 Bitmap 每一位对应一个 Slot，如果某一位为 1，表示这个 Slot 属于当前实例 实例间通信频率 Gossip 协议的工作原理第二点 实例每 100ms 扫描本地的实例列表，如果发现有实例最近一次接收 PONG 消息的时间已经大于配置项 cluster-node-timeout 的一半，就会立刻给该实例发送 PING 消息，更新这个实例上的集群状态信息 每秒会发送的 PING 消息数量 = 1 + 10 * 实例数 实例数 = 最近一次接收 PONG 消息的时间超出 cluster-node-timeout/2 降低实例间的通信开销 不能减小实例传输的消息大小 只能修改 cluster-node-timeout 配置项 默认 15 秒，调大到 20 或 25 秒 验证调整后的值是否能减少心跳消息占用的集群网络带宽 调整前后使用 tcpdump 命令抓取实例发送心跳信息网络包的情况 例如，执行：tcpdump host 192.</description>
    <pubDate>Fri, 23 Feb 2024 15:00:46 GMT</pubDate>
  </item>
    </channel>
  </rss>